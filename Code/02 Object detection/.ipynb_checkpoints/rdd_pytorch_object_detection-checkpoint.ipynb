{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFFWl3MldnEu"
   },
   "source": [
    "*based on https://towardsdatascience.com/building-your-own-object-detector-pytorch-vs-tensorflow-and-how-to-even-get-started-1d314691d4ae*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11262,
     "status": "ok",
     "timestamp": 1616335432860,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "C0bqCtgil-m_",
    "outputId": "11fa0871-c143-4cfb-f807-bc449e7b8c5c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os,sys, random\n",
    "from shutil import copy\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "!pip install --upgrade wandb\n",
    "!wandb login 1d9174288139c1b3c01e1aeed9df5ed89511e203\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2lcgNq5PFSC"
   },
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LNar7lJsAzz"
   },
   "outputs": [],
   "source": [
    "!mkdir -p \"/content/pytorch object detection\" \"/content/data\"\n",
    "!unzip -q \"/content/drive/MyDrive/Masterthesis/datasets/sinsheim_random_C/sinsheim_random_C.zip\"  -d /content/data #'/content/drive/My Drive/Masterthesis/datasets/sinsheim_random460/sinsheim_random460.zip' -d /content/data\n",
    "\n",
    "os.chdir(\"/content/data\")\n",
    "!find . -name '.DS_Store' -type f -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol9NRCaFUVxO"
   },
   "source": [
    "Create csv for dataset from .xml annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1616335445673,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "z_zKfJvqUVB7",
    "outputId": "84df5381-a264-4b69-a588-abe1f9b02586"
   },
   "outputs": [],
   "source": [
    "def xml_to_csv(path):\n",
    "  xml_list = []\n",
    "  for xml_file in glob.glob(path + '/*.xml'):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    for member in root.findall('object'):\n",
    "      value = (root.find('filename').text,\n",
    "               int(root.find('size')[0].text),\n",
    "               int(root.find('size')[1].text),\n",
    "               member[0].text,\n",
    "               int(member[4][0].text),\n",
    "               int(member[4][1].text),\n",
    "               int(member[4][2].text),\n",
    "               int(member[4][3].text)\n",
    "               )\n",
    "      xml_list.append(value)\n",
    "  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "  xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "  return xml_df\n",
    "\n",
    "image_path = os.path.join(os.getcwd(), '/content/data/annotations')\n",
    "xml_df = xml_to_csv(image_path)\n",
    "xml_df.to_csv('/content/data/labels.csv', index=None)\n",
    "print('Successfully converted xml to csv.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOZ3FBhDd0Mp"
   },
   "source": [
    "Install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRS9cgdORaoR"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/pytorch object detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4637,
     "status": "ok",
     "timestamp": 1616335454993,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "0bO1gIcHsF_Z",
    "outputId": "e07ad096-7b6f-4cb5-81a5-21a31f50c344"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/pytorch/vision.git\n",
    "cd vision\n",
    "git checkout v0.3.0\n",
    "cp references/detection/utils.py ../\n",
    "cp references/detection/transforms.py ../\n",
    "cp references/detection/coco_eval.py ../\n",
    "cp references/detection/engine.py ../\n",
    "cp references/detection/coco_utils.py ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuSlq6slSVhX"
   },
   "outputs": [],
   "source": [
    "import pycocotools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import utils\n",
    "\n",
    "import transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvDLo29psnEL"
   },
   "source": [
    "#Configuring the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Jy_hmcyd85K"
   },
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDnuJ8vvsyry"
   },
   "outputs": [],
   "source": [
    "def parse_one_annot(path_to_data_file, filename):\n",
    "  data = pd.read_csv(path_to_data_file)\n",
    "  boxes_array = data[data[\"filename\"] == filename][[\"xmin\", \"ymin\",        \n",
    "  \"xmax\", \"ymax\"]].values\n",
    "  return boxes_array\n",
    "\n",
    "class SinsheimDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, data_file, transforms=None):\n",
    "    self.root = root\n",
    "    self.transforms = transforms\n",
    "    self.imgs = sorted(os.listdir(os.path.join(root, \"images\")))\n",
    "    self.path_to_data_file = data_file\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # load images and bounding boxes\n",
    "    img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    box_list = parse_one_annot(self.path_to_data_file, \n",
    "    self.imgs[idx])\n",
    "    boxes = torch.as_tensor(box_list, dtype=torch.float32)\n",
    "  \n",
    "    num_objs = len(box_list)\n",
    "    # there is only one class\n",
    "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "    image_id = torch.tensor([idx])\n",
    "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:,0])\n",
    "    # suppose all instances are not crowd\n",
    "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "    target = {}\n",
    "    target[\"boxes\"] = boxes\n",
    "    target[\"labels\"] = labels\n",
    "    target[\"image_id\"] = image_id\n",
    "    target[\"area\"] = area\n",
    "    target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "    if self.transforms is not None:\n",
    "      img, target = self.transforms(img, target)\n",
    "\n",
    "    return img, target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1616335483404,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "nFIpRUf_sznQ",
    "outputId": "d99ed219-1b12-48e9-83be-9dae2303b4c6"
   },
   "outputs": [],
   "source": [
    "dataset = SinsheimDataset(root= \"/content/data/\",\n",
    "data_file= \"/content/data/labels.csv\")\n",
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wne-BGURtkF_"
   },
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "   # load an object detection model pre-trained on COCO\n",
    "   model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "   # get the number of input features for the classifier\n",
    "   in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "   # replace the pre-trained head with a new one\n",
    "   model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n",
    "   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gh-2k0wrvFPB"
   },
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "   transforms = []\n",
    "   # converts the image, a PIL image, into a PyTorch Tensor\n",
    "   transforms.append(T.ToTensor())\n",
    "   if train:\n",
    "      # during training, randomly flip the training images\n",
    "      # and ground-truth for data augmentation\n",
    "      transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "   return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YLuu0B0eFbt"
   },
   "source": [
    "Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1616335496972,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "R4TkbubYvPOC",
    "outputId": "c5518803-97c0-45e6-fde8-93dc945535d9"
   },
   "outputs": [],
   "source": [
    "TRAINING_SPLIT = 0.8\n",
    "\n",
    "train_split = round(len(dataset) * TRAINING_SPLIT)\n",
    "# use our dataset and defined transformations\n",
    "dataset = SinsheimDataset(root= \"/content/data\",\n",
    "                         data_file= \"/content/data/labels.csv\",\n",
    "                         transforms = get_transform(train=True))\n",
    "dataset_test = SinsheimDataset(root= \"/content/data\",\n",
    "                              data_file= \"/content/data/labels.csv\",\n",
    "                              transforms = get_transform(train=False))\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:train_split])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[train_split:])\n",
    "\n",
    "print(\"We have: {} examples, {} are training and {} testing\".format(len(indices), len(dataset), len(dataset_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VDra-Yuc0iM"
   },
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a91ba6ce63a840eeb1795ba92ce926e6",
      "9054d6bb794e4acbb91845b6d77267e6",
      "1f181a3095a74fd0a7edcb59d3405ada",
      "bcf88b2230f141609511d923aa1301de",
      "677a53c2cab14d31b0de79283663d14f",
      "2f05328353d543be926c7b98d334a658",
      "ef5b6064d1854624a564c951a6c1bba2",
      "0f2ddcd7069642218b40a39d8323e6e2"
     ]
    },
    "executionInfo": {
     "elapsed": 8702,
     "status": "ok",
     "timestamp": 1616335511112,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "CjkFXreyvsWD",
    "outputId": "3f3e0827-c0d9-4369-fd6f-6892762e09ad"
   },
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "  print(\"GPU is available\")\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print(\"GPU is not available!\")\n",
    "  \n",
    "# our dataset has two classes only - damage and not damage\n",
    "num_classes = 2\n",
    "# get the model using our helper function\n",
    "model = get_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxRafyriG2B2"
   },
   "outputs": [],
   "source": [
    "for i in model.parameters():\n",
    "  i.requires_grad = True\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k_PnFS5c4Wq"
   },
   "source": [
    "Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1616335861182,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "RMsfCetsczzC",
    "outputId": "de9a61b8-e06d-4d70-d430-8d1a64d38c85"
   },
   "outputs": [],
   "source": [
    "TRAINING_BATCH_SIZE = 2\n",
    "\n",
    "LR = 0.005\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "STEP_SIZE = 3\n",
    "GAMMA = 0.1\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "              dataset, batch_size=TRAINING_BATCH_SIZE, shuffle=True, num_workers=4,\n",
    "              collate_fn=utils.collate_fn)\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "         dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "         collate_fn=utils.collate_fn)\n",
    "\n",
    "def set_parameters(LR,MOMENTUM,WEIGHT_DECAY,STEP_SIZE,GAMMA):\n",
    "  # construct an optimizer\n",
    "  params = [p for p in model.parameters() if p.requires_grad]\n",
    "  optimizer = torch.optim.SGD(params, lr=LR,\n",
    "                              momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "  # and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
    "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=STEP_SIZE,\n",
    "                                                gamma=GAMMA)\n",
    "  return optimizer, lr_scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuwYE5ce_2XY"
   },
   "source": [
    "Engine fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6CPDNpu_ACD"
   },
   "outputs": [],
   "source": [
    "import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "import utils, math, time\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "  model.train()\n",
    "  metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "  metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "  header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "  lr_scheduler = None\n",
    "  if epoch == 0:\n",
    "    warmup_factor = 1. / 1000\n",
    "    warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "    lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "  for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    loss_dict = model(images, targets)\n",
    "\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    # reduce losses over all GPUs for logging purposes\n",
    "    loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "    losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "    loss_value = losses_reduced.item()\n",
    "\n",
    "    if not math.isfinite(loss_value):\n",
    "      print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "      print(loss_dict_reduced)\n",
    "      sys.exit(1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    wandb.log({'loss': losses_reduced, 'epoch': epoch})\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "      lr_scheduler.step()\n",
    "\n",
    "    metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "    metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "\n",
    "def _get_iou_types(model):\n",
    "  model_without_ddp = model\n",
    "  if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "    model_without_ddp = model.module\n",
    "  iou_types = [\"bbox\"]\n",
    "  if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "    iou_types.append(\"segm\")\n",
    "  if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "    iou_types.append(\"keypoints\")\n",
    "  return iou_types\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "  n_threads = torch.get_num_threads()\n",
    "  # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "  torch.set_num_threads(1)\n",
    "  cpu_device = torch.device(\"cpu\")\n",
    "  model.eval()\n",
    "  metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "  header = 'Test:'\n",
    "\n",
    "  coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "  iou_types = _get_iou_types(model)\n",
    "  coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "  for image, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "    image = list(img.to(device) for img in image)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    model_time = time.time()\n",
    "    outputs = model(image)\n",
    "\n",
    "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "    model_time = time.time() - model_time\n",
    "\n",
    "    res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "    evaluator_time = time.time()\n",
    "    coco_evaluator.update(res)\n",
    "    evaluator_time = time.time() - evaluator_time\n",
    "    metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "  # gather the stats from all processes\n",
    "  metric_logger.synchronize_between_processes()\n",
    "  print(\"Averaged stats:\", metric_logger)\n",
    "  coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "  # accumulate predictions from all images\n",
    "  coco_evaluator.accumulate()\n",
    "  coco_evaluator.summarize()\n",
    "  torch.set_num_threads(n_threads)\n",
    "  return coco_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ojprug2O-b6"
   },
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 538538,
     "status": "ok",
     "timestamp": 1616336426430,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "sCfE4naEv0eg",
    "outputId": "49139654-ce93-4f75-8fd8-b16edd066e83"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"rdd-pytorch\")\n",
    "wandb.config.update({\"Training batch size\":TRAINING_BATCH_SIZE, \n",
    "                     \"Learning rate\" : LR, \n",
    "                     \"Momentum\" : MOMENTUM, \n",
    "                     \"Weight decay\":WEIGHT_DECAY,\n",
    "                     \"Step size\":STEP_SIZE,\n",
    "                     \"Gamma\":GAMMA\n",
    "                     })\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "optimizer, lr_scheduler = set_parameters(LR,MOMENTUM,WEIGHT_DECAY,STEP_SIZE,GAMMA)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "   # train for one epoch, printing every 10 iterations\n",
    "   train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=20)\n",
    "   # update the learning rate\n",
    "   lr_scheduler.step()\n",
    "   # evaluate on the test dataset\n",
    "   evaluate(model, data_loader_test, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 541848,
     "status": "ok",
     "timestamp": 1610208703038,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "f8YLxIUQ2aEy",
    "outputId": "b6b16f55-9e5f-465e-aff8-363555ac47cd"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/Masterthesis/Colab_Notebooks/object_detection/pytorch_transfer_learning/models/model_2\")\n",
    "wandb.save(\"/content/drive/MyDrive/Masterthesis/Colab_Notebooks/object_detection/pytorch_transfer_learning/models/model_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Brl_gDHLYnd3"
   },
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOWoGgN1YrcO"
   },
   "outputs": [],
   "source": [
    "model_number = 8\n",
    "for WEIGHT_DECAY in [0.005]:\n",
    "  wandb.init(project=\"rdd-pytorch\", reinit=True)\n",
    "  for LR in [0.001,0.01,0.1]:\n",
    "    wandb.config.update({\"Training batch size\":TRAINING_BATCH_SIZE, \n",
    "                      \"Learning rate\" : LR, \n",
    "                      \"Momentum\" : MOMENTUM, \n",
    "                      \"Weight decay\":WEIGHT_DECAY,\n",
    "                      \"Step size\":STEP_SIZE,\n",
    "                      \"Gamma\":GAMMA,\n",
    "                      \"Model number\": model_number\n",
    "                      })\n",
    "    optimizer, lr_scheduler = set_parameters(LR,MOMENTUM,WEIGHT_DECAY,STEP_SIZE,GAMMA)\n",
    "    for epoch in range(10):\n",
    "      # train for one epoch, printing every 10 iterations\n",
    "      train_one_epoch(model, optimizer, data_loader, device, epoch,\n",
    "                    print_freq=10)\n",
    "      # update the learning rate\n",
    "      lr_scheduler.step()\n",
    "      # evaluate on the test dataset\n",
    "      evaluate(model, data_loader_test, device=device)\n",
    "    \n",
    "    torch.save(model.state_dict(), \"/content/drive/My Drive/Masterthesis/Colab_Notebooks/transferlearning/pytorch_transfer_learning/models/model_{}\".format(model_number))\n",
    "    model_number += 1\n",
    "  wandb.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2Rnl8ukwj04"
   },
   "source": [
    "#Predictions with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le0g0SDlBz2I"
   },
   "outputs": [],
   "source": [
    "def show_inference(idx,dataset,model_number = \"1\",threshold = 0.8):\n",
    "  img, _ = dataset_test[idx]\n",
    "  label_boxes = np.array(dataset[idx][1][\"boxes\"])\n",
    "  #put the model in evaluation mode\n",
    "  loaded_model.eval()\n",
    "  with torch.no_grad():\n",
    "    prediction = loaded_model([img])\n",
    "  image = Image.fromarray(img.mul(255).permute(1, 2,0).byte().numpy())\n",
    "  draw = ImageDraw.Draw(image)\n",
    "  # draw groundtruth\n",
    "  for elem in range(len(label_boxes)):\n",
    "    draw.rectangle([(label_boxes[elem][0], label_boxes[elem][1]),\n",
    "    (label_boxes[elem][2], label_boxes[elem][3])],\n",
    "    outline =\"green\", width =3)\n",
    "  for element in range(len(prediction[0][\"boxes\"])):\n",
    "    boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
    "    score = np.round(prediction[0][\"scores\"][element].cpu().numpy(),\n",
    "                    decimals= 4)\n",
    "    if score > threshold:\n",
    "      draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])], \n",
    "      outline =\"red\", width =3)\n",
    "      draw.text((boxes[0], boxes[1]), text = str(score))\n",
    "  image.save(\"/content/drive/MyDrive/Masterthesis/Colab_Notebooks/object_detection/pytorch_transfer_learning/models/model_{}_results/image{}.jpg\".format(model_number,idx))\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrNZcUNc28Td"
   },
   "source": [
    "Test all test-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 463905,
     "status": "ok",
     "timestamp": 1610211081544,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "hk2LzGeKPkSt",
    "outputId": "ee2eca7e-db33-4b85-f3b9-5343988d785d"
   },
   "outputs": [],
   "source": [
    "MODEL_NUMBER = 2\n",
    "\n",
    "loaded_model = get_model(num_classes = 2)\n",
    "loaded_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Masterthesis/Colab_Notebooks/object_detection/pytorch_transfer_learning/models/model_\"+str(MODEL_NUMBER)))\n",
    "\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "  os.mkdir(\"/content/drive/MyDrive/Masterthesis/Colab_Notebooks/object_detection/pytorch_transfer_learning/models/model_{}_results\".format(str(MODEL_NUMBER)))\n",
    "except:\n",
    "  pass\n",
    "for idx in tqdm(range(len(dataset_test))):\n",
    "  show_inference(idx, dataset_test, str(MODEL_NUMBER),0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnoucqUpG9RR"
   },
   "source": [
    "# Get damage features of all Sinsheim images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBEyWiK7HDU0"
   },
   "source": [
    "Load cropped images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oavdt3L4O34O"
   },
   "outputs": [],
   "source": [
    "import csv,tqdm\n",
    "\n",
    "csv_path = '/content/drive/My Drive/Masterthesis/datasets/ka_si_BC_IRI.csv'\n",
    "zip_path = '/content/drive/My\\ Drive/Masterthesis/datasets/ka_si_C.zip'\n",
    "datadir = '/content/data/'\n",
    "img_folder_path = os.path.join(datadir,zip_path.split(\"/\")[-1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyoKhvSVG8Yd"
   },
   "outputs": [],
   "source": [
    "!unzip -q $zip_path -d $datadir\n",
    "os.chdir(datadir)\n",
    "!find . -name '.DS_Store' -type f -delete\n",
    "os.chdir(\"/content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2755MDLtCUV3"
   },
   "source": [
    "Extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZraXM9ODg2i"
   },
   "outputs": [],
   "source": [
    "def calculate_diag(box):\n",
    "  diag = np.sqrt((box[0]-box[2])**2 + (box[1]-box[3])**2)\n",
    "  return diag\n",
    "\n",
    "def get_features(pred_result,threshold = 0.5):\n",
    "  sum_diagonals = 0\n",
    "  num_damages = 0\n",
    "  num_horizontal = 0\n",
    "  num_vertical = 0\n",
    "  sum_horizontal = 0\n",
    "  sum_vertical = 0 \n",
    "  scores = pred_result[\"scores\"].tolist()\n",
    "  boxes = pred_result[\"boxes\"].tolist()\n",
    "  for i,score in enumerate(scores):\n",
    "    if score >= threshold:\n",
    "      num_damages += 1\n",
    "      sum_diagonals += calculate_diag(boxes[i])\n",
    "      #print(boxes[i])\n",
    "      horizontal = abs(boxes[i][0] - boxes[i][2])\n",
    "      vertical = abs(boxes[i][1] - boxes[i][3])\n",
    "      if horizontal >= 7*vertical:\n",
    "        num_horizontal += 1\n",
    "        sum_horizontal += horizontal\n",
    "      elif vertical >= 7*horizontal:\n",
    "        num_vertical += 1\n",
    "        sum_vertical += vertical\n",
    "  return num_damages, int(sum_diagonals), num_horizontal, num_vertical, sum_horizontal, sum_vertical\n",
    "  \n",
    "def read_csv(path):\n",
    "  with open(path, mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    data_list = [rows for rows in reader]\n",
    "  return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 428624,
     "status": "ok",
     "timestamp": 1610215077362,
     "user": {
      "displayName": "Christian Seeger",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhBdZSahLB91Z8XWEq8kZMwE7a1EhKVD2OWNPFGH6k=s64",
      "userId": "02533968630071158910"
     },
     "user_tz": -60
    },
    "id": "BoEqUKorS_uF",
    "outputId": "c7a1a3d2-0b30-45e3-edce-70b60324ff47"
   },
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "csv_list = read_csv(csv_path)\n",
    "model = loaded_model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for img_name,iri_val,_ in tqdm.tqdm(csv_list):\n",
    "    img_path = os.path.join(img_folder_path,img_name)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img,_ = T.ToTensor()(img,\"\")\n",
    "    img = img.unsqueeze(0).to('cuda')\n",
    "    prediction = model(img)\n",
    "    num_damages, sum_diagonals, num_horizontal, num_vertical, sum_horizontal, sum_vertical = get_features(prediction[0])\n",
    "    feature_list.append([img_name,iri_val,num_damages,sum_diagonals, num_horizontal, num_vertical, sum_horizontal, sum_vertical])\n",
    "    \n",
    "features_df = pd.DataFrame(feature_list)\n",
    "features_df.to_csv(\"ka_si_BC_IRI_annotated.csv\", index = False,header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIIBc9ZgWFuZ"
   },
   "source": [
    "### Save state dict for imge classification with IRI\n",
    "!!! This step changes the model !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipaiz7wywLGL"
   },
   "outputs": [],
   "source": [
    "!mkdir -p \"/content/models/\"\n",
    "\n",
    "model_backbone = model.backbone.body\n",
    "model_backbone.add_module('avgpool',nn.AdaptiveAvgPool2d(output_size=(1,1)))\n",
    "model_backbone.add_module('fc',nn.Linear(2048,1000,True))\n",
    "torch.save(model_backbone.state_dict(), \"/content/drive/MyDrive/Masterthesis/datasets/trained_resnet50_backbone_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8c3qX5mLc0-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMsBty9kg6r/3Ce7ag3YKGl",
   "collapsed_sections": [],
   "mount_file_id": "1UQpBjxhJgUvPrNuRWN4BSqpxztadROy-",
   "name": "rdd_pytorch_object_detection.ipynb",
   "provenance": [
    {
     "file_id": "1wt2LuKvAdSynLyhWkvg7FTBYsbz1c4Df",
     "timestamp": 1597680476449
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f2ddcd7069642218b40a39d8323e6e2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f181a3095a74fd0a7edcb59d3405ada": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f05328353d543be926c7b98d334a658",
      "max": 167502836,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_677a53c2cab14d31b0de79283663d14f",
      "value": 167502836
     }
    },
    "2f05328353d543be926c7b98d334a658": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "677a53c2cab14d31b0de79283663d14f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9054d6bb794e4acbb91845b6d77267e6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a91ba6ce63a840eeb1795ba92ce926e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f181a3095a74fd0a7edcb59d3405ada",
       "IPY_MODEL_bcf88b2230f141609511d923aa1301de"
      ],
      "layout": "IPY_MODEL_9054d6bb794e4acbb91845b6d77267e6"
     }
    },
    "bcf88b2230f141609511d923aa1301de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f2ddcd7069642218b40a39d8323e6e2",
      "placeholder": "​",
      "style": "IPY_MODEL_ef5b6064d1854624a564c951a6c1bba2",
      "value": " 160M/160M [00:00&lt;00:00, 219MB/s]"
     }
    },
    "ef5b6064d1854624a564c951a6c1bba2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
