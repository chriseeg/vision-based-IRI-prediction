{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rdd_pytorch_object_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtK2-7g7yty"
      },
      "source": [
        "# Automatic road damage detection \n",
        "via PyTorch object detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFFWl3MldnEu"
      },
      "source": [
        "*based on https://towardsdatascience.com/building-your-own-object-detector-pytorch-vs-tensorflow-and-how-to-even-get-started-1d314691d4ae*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0bqCtgil-m_"
      },
      "source": [
        "import os,sys, random\n",
        "from shutil import copy\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "!pip install --upgrade wandb\n",
        "!wandb login (wandb credentials)\n",
        "\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2lcgNq5PFSC"
      },
      "source": [
        "Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LNar7lJsAzz"
      },
      "source": [
        "!mkdir -p \"/content/pytorch object detection\" \"/content/data\"\n",
        "!unzip -q \"/content/drive/MyDrive/Masterthesis/Data/02\\ Object\\ detection/sinsheim object detection.zip\"  -d /content/data\n",
        "\n",
        "os.chdir(\"/content/data\")\n",
        "!find . -name '.DS_Store' -type f -delete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol9NRCaFUVxO"
      },
      "source": [
        "Create csv for dataset from .xml annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_zKfJvqUVB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84df5381-a264-4b69-a588-abe1f9b02586"
      },
      "source": [
        "def xml_to_csv(path):\n",
        "  xml_list = []\n",
        "  for xml_file in glob.glob(path + '/*.xml'):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    for member in root.findall('object'):\n",
        "      value = (root.find('filename').text,\n",
        "               int(root.find('size')[0].text),\n",
        "               int(root.find('size')[1].text),\n",
        "               member[0].text,\n",
        "               int(member[4][0].text),\n",
        "               int(member[4][1].text),\n",
        "               int(member[4][2].text),\n",
        "               int(member[4][3].text)\n",
        "               )\n",
        "      xml_list.append(value)\n",
        "  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "  xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "  return xml_df\n",
        "\n",
        "image_path = os.path.join(os.getcwd(), '/content/data/annotations')\n",
        "xml_df = xml_to_csv(image_path)\n",
        "xml_df.to_csv('/content/data/labels.csv', index=None)\n",
        "print('Successfully converted xml to csv.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully converted xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOZ3FBhDd0Mp"
      },
      "source": [
        "Install torchvision, used for object detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRS9cgdORaoR"
      },
      "source": [
        "os.chdir(\"/content/pytorch object detection\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bO1gIcHsF_Z"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuSlq6slSVhX"
      },
      "source": [
        "import pycocotools\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import utils\n",
        "\n",
        "import transforms as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvDLo29psnEL"
      },
      "source": [
        "#Configuring the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jy_hmcyd85K"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDnuJ8vvsyry"
      },
      "source": [
        "def parse_one_annot(path_to_data_file, filename):\n",
        "  # parse the bounding-box information in .csv file\n",
        "  data = pd.read_csv(path_to_data_file)\n",
        "  boxes_array = data[data[\"filename\"] == filename][[\"xmin\", \"ymin\",        \n",
        "  \"xmax\", \"ymax\"]].values\n",
        "  return boxes_array\n",
        "\n",
        "class SinsheimDataset(torch.utils.data.Dataset):\n",
        "  # Custom class for the dataset\n",
        "  def __init__(self, root, data_file, transforms=None):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    self.imgs = sorted(os.listdir(os.path.join(root, \"images\")))\n",
        "    self.path_to_data_file = data_file\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # load image\n",
        "    img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "    # open image and convert to RGB\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    # get bounding-boxes of image\n",
        "    box_list = parse_one_annot(self.path_to_data_file, self.imgs[idx])\n",
        "    boxes = torch.as_tensor(box_list, dtype=torch.float32)\n",
        "  \n",
        "    num_objs = len(box_list)\n",
        "    # there is only one class\n",
        "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:,0])\n",
        "    # suppose all instances are not crowd\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "    # Transform images\n",
        "    if self.transforms is not None:\n",
        "      img, target = self.transforms(img, target)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wne-BGURtkF_"
      },
      "source": [
        "def get_model(num_classes):\n",
        "   # load an object detection model pre-trained on COCO\n",
        "   model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "   # get the number of input features for the classifier\n",
        "   in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "   # replace the pre-trained head with a new one\n",
        "   model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n",
        "   return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh-2k0wrvFPB"
      },
      "source": [
        "def get_transform(train):\n",
        "   transforms = []\n",
        "   # converts the image, a PIL image, into a PyTorch Tensor\n",
        "   transforms.append(T.ToTensor())\n",
        "   if train:\n",
        "      # during training, randomly flip the training images\n",
        "      # and ground-truth for data augmentation\n",
        "      transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "   return T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YLuu0B0eFbt"
      },
      "source": [
        "# Preparing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4TkbubYvPOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5518803-97c0-45e6-fde8-93dc945535d9"
      },
      "source": [
        "TRAINING_SPLIT = 0.8\n",
        "\n",
        "train_split = round(len(dataset) * TRAINING_SPLIT)\n",
        "# use our dataset and defined transformations\n",
        "dataset = SinsheimDataset(root= \"/content/data\",\n",
        "                         data_file= \"/content/data/labels.csv\",\n",
        "                         transforms = get_transform(train=True))\n",
        "dataset_test = SinsheimDataset(root= \"/content/data\",\n",
        "                              data_file= \"/content/data/labels.csv\",\n",
        "                              transforms = get_transform(train=False))\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:train_split])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[train_split:])\n",
        "\n",
        "print(\"We have: {} examples, {} are training and {} testing\".format(len(indices), len(dataset), len(dataset_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have: 597 examples, 478 are training and 119 testing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VDra-Yuc0iM"
      },
      "source": [
        "Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjkFXreyvsWD"
      },
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print(\"GPU is available\")\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print(\"GPU is not available!\")\n",
        "  \n",
        "# our dataset has two classes only - damage and not damage\n",
        "num_classes = 2\n",
        "# get the model using our helper function\n",
        "model = get_model(num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k_PnFS5c4Wq"
      },
      "source": [
        "Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMsfCetsczzC"
      },
      "source": [
        "TRAINING_BATCH_SIZE = 2\n",
        "\n",
        "LR = 0.005\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.0005\n",
        "\n",
        "STEP_SIZE = 3\n",
        "GAMMA = 0.1\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "              dataset, batch_size=TRAINING_BATCH_SIZE, shuffle=True, num_workers=4,\n",
        "              collate_fn=utils.collate_fn)\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "         dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "         collate_fn=utils.collate_fn)\n",
        "\n",
        "def set_parameters(LR,MOMENTUM,WEIGHT_DECAY,STEP_SIZE,GAMMA):\n",
        "  # construct an optimizer\n",
        "  params = [p for p in model.parameters() if p.requires_grad]\n",
        "  optimizer = torch.optim.SGD(params, lr=LR,\n",
        "                              momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "  # construct learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=STEP_SIZE,\n",
        "                                                gamma=GAMMA)\n",
        "  return optimizer, lr_scheduler\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuwYE5ce_2XY"
      },
      "source": [
        "Engine fuctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6CPDNpu_ACD"
      },
      "source": [
        "import torchvision.models.detection.mask_rcnn\n",
        "\n",
        "from coco_utils import get_coco_api_from_dataset\n",
        "from coco_eval import CocoEvaluator\n",
        "import utils, math, time\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
        "  model.train()\n",
        "  metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "  metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "  header = 'Epoch: [{}]'.format(epoch)\n",
        "\n",
        "  lr_scheduler = None\n",
        "  if epoch == 0:\n",
        "    warmup_factor = 1. / 1000\n",
        "    warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "    lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
        "\n",
        "  for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "    images = list(image.to(device) for image in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "    # predict and calculate loss\n",
        "    loss_dict = model(images, targets)\n",
        "\n",
        "    losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "    # reduce losses over all GPUs for logging purposes\n",
        "    loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "    losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "    loss_value = losses_reduced.item()\n",
        "\n",
        "    if not math.isfinite(loss_value):\n",
        "      print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "      print(loss_dict_reduced)\n",
        "      sys.exit(1)\n",
        "    \n",
        "    # adapt model parameters\n",
        "    optimizer.zero_grad()\n",
        "    losses.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    wandb.log({'loss': losses_reduced, 'epoch': epoch})\n",
        "\n",
        "    if lr_scheduler is not None:\n",
        "      lr_scheduler.step()\n",
        "\n",
        "    metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "    metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "\n",
        "def _get_iou_types(model):\n",
        "  model_without_ddp = model\n",
        "  if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "    model_without_ddp = model.module\n",
        "  iou_types = [\"bbox\"]\n",
        "  if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "    iou_types.append(\"segm\")\n",
        "  if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
        "    iou_types.append(\"keypoints\")\n",
        "  return iou_types\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "  # evaluate training\n",
        "  n_threads = torch.get_num_threads()\n",
        "  torch.set_num_threads(1)\n",
        "  cpu_device = torch.device(\"cpu\")\n",
        "  model.eval()\n",
        "  metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "  header = 'Test:'\n",
        "\n",
        "  coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "  iou_types = _get_iou_types(model)\n",
        "  coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "  for image, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "    image = list(img.to(device) for img in image)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    model_time = time.time()\n",
        "    outputs = model(image)\n",
        "\n",
        "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "    model_time = time.time() - model_time\n",
        "\n",
        "    res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "    evaluator_time = time.time()\n",
        "    coco_evaluator.update(res)\n",
        "    evaluator_time = time.time() - evaluator_time\n",
        "    metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "  # gather the stats from all processes\n",
        "  metric_logger.synchronize_between_processes()\n",
        "  print(\"Averaged stats:\", metric_logger)\n",
        "  coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "  # accumulate predictions from all images\n",
        "  coco_evaluator.accumulate()\n",
        "  coco_evaluator.summarize()\n",
        "  torch.set_num_threads(n_threads)\n",
        "  return coco_evaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ojprug2O-b6"
      },
      "source": [
        "# Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCfE4naEv0eg"
      },
      "source": [
        "wandb.init(project=\"rdd-pytorch\")\n",
        "wandb.config.update({\"Training batch size\":TRAINING_BATCH_SIZE, \n",
        "                     \"Learning rate\" : LR, \n",
        "                     \"Momentum\" : MOMENTUM, \n",
        "                     \"Weight decay\":WEIGHT_DECAY,\n",
        "                     \"Step size\":STEP_SIZE,\n",
        "                     \"Gamma\":GAMMA\n",
        "                     })\n",
        "\n",
        "num_epochs = 10\n",
        "optimizer, lr_scheduler = set_parameters(LR,MOMENTUM,WEIGHT_DECAY,STEP_SIZE,GAMMA)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "   # train for one epoch, printing every 10 iterations\n",
        "   train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=20)\n",
        "   # update the learning rate\n",
        "   lr_scheduler.step()\n",
        "   # evaluate on the test dataset\n",
        "   evaluate(model, data_loader_test, device=device)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8YLxIUQ2aEy"
      },
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Masterthesis/Colab_Notebooks/object_detection/pytorch_transfer_learning/models/model_2\")\n",
        "wandb.save(\"/content/drive/MyDrive/Masterthesis/Colab_Notebooks/object_detection/pytorch_transfer_learning/models/model_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brl_gDHLYnd3"
      },
      "source": [
        "### Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOWoGgN1YrcO"
      },
      "source": [
        "model_number = 0\n",
        "for WEIGHT_DECAY in [0.005]:\n",
        "  wandb.init(project=\"rdd-pytorch\", reinit=True)\n",
        "  for LR in [0.001,0.01,0.1]:\n",
        "    wandb.config.update({\"Training batch size\":TRAINING_BATCH_SIZE, \n",
        "                      \"Learning rate\" : LR, \n",
        "                      \"Momentum\" : MOMENTUM, \n",
        "                      \"Weight decay\":WEIGHT_DECAY,\n",
        "                      \"Step size\":STEP_SIZE,\n",
        "                      \"Gamma\":GAMMA,\n",
        "                      \"Model number\": model_number\n",
        "                      })\n",
        "    optimizer, lr_scheduler = set_parameters(LR,MOMENTUM,WEIGHT_DECAY,STEP_SIZE,GAMMA)\n",
        "    for epoch in range(10):\n",
        "      # train for one epoch, printing every 10 iterations\n",
        "      train_one_epoch(model, optimizer, data_loader, device, epoch,\n",
        "                    print_freq=10)\n",
        "      # update the learning rate\n",
        "      lr_scheduler.step()\n",
        "      # evaluate on the test dataset\n",
        "      evaluate(model, data_loader_test, device=device)\n",
        "    \n",
        "    torch.save(model.state_dict(), \"/content/drive/My Drive/Masterthesis/Colab_Notebooks/transferlearning/pytorch_transfer_learning/models/model_{}\".format(model_number))\n",
        "    model_number += 1\n",
        "  wandb.join()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2Rnl8ukwj04"
      },
      "source": [
        "#Predictions with the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le0g0SDlBz2I"
      },
      "source": [
        "def show_inference(idx,dataset,model_number = \"1\",threshold = 0.8):\n",
        "  img, _ = dataset_test[idx]\n",
        "  label_boxes = np.array(dataset[idx][1][\"boxes\"])\n",
        "  #put the model in evaluation mode\n",
        "  loaded_model.eval()\n",
        "  with torch.no_grad():\n",
        "    prediction = loaded_model([img])\n",
        "  image = Image.fromarray(img.mul(255).permute(1, 2,0).byte().numpy())\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  # draw groundtruth\n",
        "  for elem in range(len(label_boxes)):\n",
        "    draw.rectangle([(label_boxes[elem][0], label_boxes[elem][1]),\n",
        "    (label_boxes[elem][2], label_boxes[elem][3])],\n",
        "    outline =\"green\", width =3)\n",
        "  for element in range(len(prediction[0][\"boxes\"])):\n",
        "    boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
        "    score = np.round(prediction[0][\"scores\"][element].cpu().numpy(),\n",
        "                    decimals= 4)\n",
        "    if score > threshold:\n",
        "      draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])], \n",
        "      outline =\"red\", width =3)\n",
        "      draw.text((boxes[0], boxes[1]), text = str(score))\n",
        "  image.save(\"/content/drive/MyDrive/Masterthesis/Colab_Notebooks/object_detection/pytorch_transfer_learning/models/model_{}_results/image{}.jpg\".format(model_number,idx))\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrNZcUNc28Td"
      },
      "source": [
        "Test all test-images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk2LzGeKPkSt"
      },
      "source": [
        "MODEL_NUMBER = 2\n",
        "\n",
        "loaded_model = get_model(num_classes = 2)\n",
        "loaded_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Masterthesis/Colab_Notebooks/object_detection/pytorch_transfer_learning/models/model_\"+str(MODEL_NUMBER)))\n",
        "\n",
        "from tqdm import tqdm\n",
        "try:\n",
        "  os.mkdir(\"/content/drive/MyDrive/Masterthesis/Colab_Notebooks/object_detection/pytorch_transfer_learning/models/model_{}_results\".format(str(MODEL_NUMBER)))\n",
        "except:\n",
        "  pass\n",
        "for idx in tqdm(range(len(dataset_test))):\n",
        "  show_inference(idx, dataset_test, str(MODEL_NUMBER),0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnoucqUpG9RR"
      },
      "source": [
        "# Get damage features of all Sinsheim images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBEyWiK7HDU0"
      },
      "source": [
        "Load cropped images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oavdt3L4O34O"
      },
      "source": [
        "import csv,tqdm\n",
        "\n",
        "csv_path = '/content/drive/My Drive/Masterthesis/datasets/ka_si_BC_IRI.csv'\n",
        "zip_path = '/content/drive/My\\ Drive/Masterthesis/datasets/ka_si_C.zip'\n",
        "datadir = '/content/data/'\n",
        "img_folder_path = os.path.join(datadir,zip_path.split(\"/\")[-1].split(\".\")[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyoKhvSVG8Yd"
      },
      "source": [
        "!unzip -q $zip_path -d $datadir\n",
        "os.chdir(datadir)\n",
        "!find . -name '.DS_Store' -type f -delete\n",
        "os.chdir(\"/content\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2755MDLtCUV3"
      },
      "source": [
        "Extract damage features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZraXM9ODg2i"
      },
      "source": [
        "def calculate_diag(box):\n",
        "  diag = np.sqrt((box[0]-box[2])**2 + (box[1]-box[3])**2)\n",
        "  return diag\n",
        "\n",
        "def get_features(pred_result,threshold = 0.5):\n",
        "  sum_diagonals = 0\n",
        "  num_damages = 0\n",
        "  num_horizontal = 0\n",
        "  num_vertical = 0\n",
        "  sum_horizontal = 0\n",
        "  sum_vertical = 0 \n",
        "  scores = pred_result[\"scores\"].tolist()\n",
        "  boxes = pred_result[\"boxes\"].tolist()\n",
        "  for i,score in enumerate(scores):\n",
        "    if score >= threshold:\n",
        "      num_damages += 1\n",
        "      sum_diagonals += calculate_diag(boxes[i])\n",
        "      #print(boxes[i])\n",
        "      horizontal = abs(boxes[i][0] - boxes[i][2])\n",
        "      vertical = abs(boxes[i][1] - boxes[i][3])\n",
        "      if horizontal >= 7*vertical:\n",
        "        num_horizontal += 1\n",
        "        sum_horizontal += horizontal\n",
        "      elif vertical >= 7*horizontal:\n",
        "        num_vertical += 1\n",
        "        sum_vertical += vertical\n",
        "  return num_damages, int(sum_diagonals), num_horizontal, num_vertical, sum_horizontal, sum_vertical\n",
        "  \n",
        "def read_csv(path):\n",
        "  with open(path, mode='r') as infile:\n",
        "    reader = csv.reader(infile)\n",
        "    data_list = [rows for rows in reader]\n",
        "  return data_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoEqUKorS_uF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a1a3d2-0b30-45e3-edce-70b60324ff47"
      },
      "source": [
        "feature_list = []\n",
        "csv_list = read_csv(csv_path)\n",
        "model = loaded_model.cuda()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for img_name,iri_val,_ in tqdm.tqdm(csv_list):\n",
        "    img_path = os.path.join(img_folder_path,img_name)\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img,_ = T.ToTensor()(img,\"\")\n",
        "    img = img.unsqueeze(0).to('cuda')\n",
        "    # predict damages\n",
        "    prediction = model(img)\n",
        "    num_damages, sum_diagonals, num_horizontal, num_vertical, sum_horizontal, sum_vertical = get_features(prediction[0])\n",
        "    feature_list.append([img_name,iri_val,num_damages,sum_diagonals, num_horizontal, num_vertical, sum_horizontal, sum_vertical])\n",
        "    \n",
        "features_df = pd.DataFrame(feature_list)\n",
        "features_df.to_csv(\"ka_si_BC_IRI_annotated.csv\", index = False,header = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9163/9163 [07:07<00:00, 21.43it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIIBc9ZgWFuZ"
      },
      "source": [
        "### Save state dict for image classification with IRI\n",
        "!!! This step changes the model !!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipaiz7wywLGL"
      },
      "source": [
        "!mkdir -p \"/content/models/\"\n",
        "\n",
        "model_backbone = model.backbone.body\n",
        "model_backbone.add_module('avgpool',nn.AdaptiveAvgPool2d(output_size=(1,1)))\n",
        "model_backbone.add_module('fc',nn.Linear(2048,1000,True))\n",
        "torch.save(model_backbone.state_dict(), \"/content/drive/MyDrive/Masterthesis/datasets/trained_resnet50_backbone_2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8c3qX5mLc0-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}